{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Server-FND.ipynb","provenance":[{"file_id":"1m2VBmFN7UsLiSqCoADrdEENM0IbCT15a","timestamp":1617703305332},{"file_id":"1hmMy8NZSaZYkgwSRV1i5Wu-cAPAEKVhG","timestamp":1610186355750},{"file_id":"1F5PZgRlqR3JgUC1m2bIuufjOKiA7oQwQ","timestamp":1610121221698},{"file_id":"1LkKZsHMQ1eWsey7UCE-EeuNHhaez5Vh6","timestamp":1610033395189},{"file_id":"1lT4HqBiWWq1y4PvoHm21MBtgTuXJpJnb","timestamp":1609759658071}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NpPYHgxRO7KO","executionInfo":{"status":"ok","timestamp":1620914933290,"user_tz":-330,"elapsed":40795,"user":{"displayName":"Viren Kathiriya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVzqjQWWgJbKaLmG0OVHY6kjVbYoEWYa-NjveK0hs=s64","userId":"10657160422125025312"}},"outputId":"c3b3d7a6-1714-45e1-9e99-be874e33ca6b"},"source":["!pip install empath\n","import pickle\n","import numpy as np\n","import nltk \n","import string \n","import re \n","import inflect \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import base64\n","\n","from sklearn.decomposition import NMF\n","from sklearn.pipeline import Pipeline\n","from sklearn.externals import joblib \n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.neighbors import KNeighborsClassifier\n","from pprint import pprint\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.model_selection import ShuffleSplit\n","from sklearn.model_selection import train_test_split\n","from nltk.corpus import stopwords \n","from nltk.tokenize import word_tokenize \n","from empath import Empath\n","from nltk.stem.porter import PorterStemmer \n","from nltk.tokenize import word_tokenize \n","from nltk.stem import WordNetLemmatizer \n","from nltk.tokenize import word_tokenize \n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier \n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble.bagging import BaggingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.ensemble.forest import ExtraTreesClassifier\n","import time\n","from sklearn import metrics \n","from sklearn.linear_model import LogisticRegression \n","from sklearn.neighbors import KNeighborsClassifier\n","from wordcloud import WordCloud, STOPWORDS \n","import matplotlib.pyplot as plt \n","import pandas as pd\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer() \n","lexicon = Empath()\n","p = inflect.engine() \n","\n","import zipfile\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/pickles.zip\", 'r')\n","zip_ref.extractall(\"./\")\n","\n","regE = joblib.load('regressorEmpath.pkl')  \n","regTS = joblib.load('regressorTS20.pkl')\n","regHy = joblib.load('regressorHybrid.pkl')\n","gbcE = joblib.load('gbcEmpath.pkl')  \n","gbcTS = joblib.load('gbcTS20.pkl')\n","gbcHy = joblib.load('gbcHybrid.pkl')\n","\n","# Convert to lowercase\n","def text_lowercase(text): \n","\treturn text.lower() \n","  \n","# convert number into words \n","def convert_number(text): \n","    # split string into list of words \n","    temp_str = text.split() \n","    # initialise empty list \n","    new_string = [] \n","  \n","    for word in temp_str: \n","        # if word is a digit, convert the digit \n","        # to numbers and append into the new_string list \n","        if word.isdigit(): \n","            temp = p.number_to_words(word) \n","            new_string.append(temp) \n","  \n","        # append the word as it is \n","        else: \n","            new_string.append(word) \n","  \n","    # join the words of new_string to form a string \n","    temp_str = ' '.join(new_string) \n","    return temp_str \n","\n","# remove punctuation \n","def remove_punctuation(text): \n","    translator = str.maketrans('', '', string.punctuation) \n","    return text.translate(translator) \n","\n","# remove whitespace from text \n","def remove_whitespace(text): \n","    return  \" \".join(text.split()) \n","  \n","# remove stopwords function \n","def remove_stopwords(text): \n","    stop_words = set(stopwords.words(\"english\")) \n","    word_tokens = word_tokenize(text) \n","    filtered_text = [word for word in word_tokens if word not in stop_words] \n","    return ' '.join(filtered_text)\n","\n","# stem words in the list of tokenised words \n","def stem_words(text): \n","    word_tokens = word_tokenize(text) \n","    stems = [stemmer.stem(word) for word in word_tokens] \n","    return ' '.join(stems)\n","\n","# lemmatize string \n","def lemmatize_word(text): \n","    word_tokens = word_tokenize(text) \n","    # provide context i.e. part-of-speech \n","    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n","    return ' '.join(lemmas)\n","\n","def empath_feature_calc(text):\n","    try:\n","        return lexicon.analyze(text,normalize=True)\n","    except:\n","        return None\n","\n","def empath_feature_calc(text):\n","    try:\n","        return lexicon.analyze(text,normalize=True)\n","    except:\n","        print(\"gajab bejjati he\")\n","        return None\n","\n","def fun(res):\n","  print(res)\n","  if res == 0:\n","    return \"Fake\"\n","  return \"True\"\n","\n","def predictEmpath(text, algorithm):\n","  df = pd.DataFrame([[text]],\n","     index=[0],\n","     columns=['text'])\n","  df['text']=df['text'].apply(text_lowercase)\n","  df['text']=df['text'].apply(convert_number)\n","  df['text']=df['text'].apply(remove_punctuation)\n","  df['text']=df['text'].apply(remove_whitespace)\n","  df['text']=df['text'].apply(remove_stopwords)\n","  df['text']=df['text'].apply(stem_words)\n","  df['text']=df['text'].apply(lemmatize_word)\n","\n","  df['empath_features'] = df['text'].apply(empath_feature_calc)\n","\n","  newDF = pd.DataFrame(columns = list(df.empath_features[0].keys()))\n","  exclude_idx = []\n","  for i in range(df.shape[0]):\n","      dict_new_dataframe = df.empath_features[i]\n","      try:\n","          new_df = pd.DataFrame.from_dict(dict_new_dataframe.items()).transpose()\n","          header_new_dataframe = new_df.iloc[0]  # 0th index is header\n","          new_df = new_df[1:]  # fetch data except header\n","          new_df.columns = header_new_dataframe #set header as dataframe header in pandas\n","          newDF = newDF.append(new_df, ignore_index=True) # populate new dataframe with new data\n","      except:\n","          exclude_idx.append(i)\n","          pass\n","\n","  columns=newDF.columns[:]\n","  x1=newDF[columns]\n","  print(x1)\n","  if algorithm == \"Regressor\":\n","    return fun(regE.predict(x1)[0])\n","  return fun(gbcE.predict(x1)[0])\n","\n","def predictTS20(text, algorithm):\n","  df = pd.DataFrame([[text]],\n","     index=[0],\n","     columns=['text'])\n","  df['text']=df['text'].apply(text_lowercase)\n","  df['text']=df['text'].apply(convert_number)\n","  df['text']=df['text'].apply(remove_punctuation)\n","  df['text']=df['text'].apply(remove_whitespace)\n","  df['text']=df['text'].apply(remove_stopwords)\n","  df['text']=df['text'].apply(stem_words)\n","  df['text']=df['text'].apply(lemmatize_word)\n","  vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True, ngram_range=(1,1))\n","  tfidf = vectorizer.fit_transform(df['text'])\n","  n_topics = 20\n","  nmf = NMF(n_components=n_topics,random_state=0)\n","  topics = nmf.fit_transform(tfidf)\n","  top_n_words = 5\n","  t_words, word_strengths = {}, {}\n","  for t_id, t in enumerate(nmf.components_):\n","      t_words[t_id] = [vectorizer.get_feature_names()[i] for i in t.argsort()[:-top_n_words - 1:-1]]\n","      word_strengths[t_id] = t[t.argsort()[:-top_n_words - 1:-1]]\n","  pipe = Pipeline([\n","    ('tfidf', vectorizer),\n","    ('nmf', nmf)\n","  ])\n","  t = pipe.transform(df['text']) \n","  t = pd.DataFrame(t, columns=[str(t_words[i]) for i in range(0, n_topics)])\n","\n","  if algorithm == \"Regressor\":\n","    return fun(regTS.predict(t)[0])\n","  return fun(gbcTS.predict(t)[0])\n","\n","def predictHybrid(text, algorithm):\n","  df = pd.DataFrame([[text]],\n","     index=[0],\n","     columns=['text'])\n","  df['text']=df['text'].apply(text_lowercase)\n","  df['text']=df['text'].apply(convert_number)\n","  df['text']=df['text'].apply(remove_punctuation)\n","  df['text']=df['text'].apply(remove_whitespace)\n","  df['text']=df['text'].apply(remove_stopwords)\n","  df['text']=df['text'].apply(stem_words)\n","  df['text']=df['text'].apply(lemmatize_word)\n","\n","  df['empath_features'] = df['text'].apply(empath_feature_calc)\n","\n","  newDF = pd.DataFrame(columns = list(df.empath_features[0].keys()))\n","  exclude_idx = []\n","  for i in range(df.shape[0]):\n","      dict_new_dataframe = df.empath_features[i]\n","      try:\n","          new_df = pd.DataFrame.from_dict(dict_new_dataframe.items()).transpose()\n","          header_new_dataframe = new_df.iloc[0]  # 0th index is header\n","          new_df = new_df[1:]  # fetch data except header\n","          new_df.columns = header_new_dataframe #set header as dataframe header in pandas\n","          newDF = newDF.append(new_df, ignore_index=True) # populate new dataframe with new data\n","      except:\n","          exclude_idx.append(i)\n","          pass\n","  \n","  vectorizer = TfidfVectorizer(stop_words=\"english\", use_idf=True, ngram_range=(1,1))\n","  tfidf = vectorizer.fit_transform(df['text'])\n","  n_topics = 20\n","  nmf = NMF(n_components=n_topics,random_state=0)\n","  topics = nmf.fit_transform(tfidf)\n","  top_n_words = 5\n","  t_words, word_strengths = {}, {}\n","  for t_id, t in enumerate(nmf.components_):\n","      t_words[t_id] = [vectorizer.get_feature_names()[i] for i in t.argsort()[:-top_n_words - 1:-1]]\n","      word_strengths[t_id] = t[t.argsort()[:-top_n_words - 1:-1]]\n","  pipe = Pipeline([\n","    ('tfidf', vectorizer),\n","    ('nmf', nmf)\n","  ])\n","  t = pipe.transform(df['text']) \n","  t = pd.DataFrame(t, columns=[str(t_words[i]) for i in range(0, n_topics)])\n","  merged=pd.concat([t, newDF], axis=1)\n","\n","  columns=merged.columns[:]\n","  x0=merged[columns]\n","  if algorithm == \"Regressor\":\n","    return fun(regHy.predict(x0)[0])\n","  return fun(gbcHy.predict(x0)[0])\n","\n","import base64\n","def stringToBase64(s):\n","    return base64.b64encode(s.encode('utf-8'))\n","\n","def base64ToString(b):\n","    return base64.b64decode(b).decode('utf-8')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting empath\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/84/a5de61a99252f60d705d7982b3648db517a704c89fa7629d3d3637a6e604/empath-0.89.tar.gz (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from empath) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->empath) (3.0.4)\n","Building wheels for collected packages: empath\n","  Building wheel for empath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for empath: filename=empath-0.89-cp37-none-any.whl size=57824 sha256=039570af764bb599f6c398b75aeee7748dcfcebe4f4db6550189ee565fab78e7\n","  Stored in directory: /root/.cache/pip/wheels/84/ea/2f/2bc54d4f9985ce61753ebc5b00cb2df51d855589267c667308\n","Successfully built empath\n","Installing collected packages: empath\n","Successfully installed empath-0.89\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3dzVv6EPVSD","executionInfo":{"status":"ok","timestamp":1620915483249,"user_tz":-330,"elapsed":546424,"user":{"displayName":"Viren Kathiriya","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVzqjQWWgJbKaLmG0OVHY6kjVbYoEWYa-NjveK0hs=s64","userId":"10657160422125025312"}},"outputId":"8c837299-055d-46b3-bdb2-75e6e7998865"},"source":["!pip install flask-ngrok\n","from flask import request,make_response\n","from flask import Flask\n","from flask_ngrok import run_with_ngrok\n","app = Flask(__name__)\n","run_with_ngrok(app) \n","import json\n","@app.route(\"/\")\n","def predict():\n","  algo = request.args.get('algo')\n","  granularity = request.args.get('granularity')\n","  text = base64ToString(request.args.get('text'))\n","  print(text)\n","  print(algo)\n","  if granularity == \"Fine\" :\n","      ans = predictEmpath(text, algo)\n","  elif granularity == \"Coarse\":\n","      ans = predictTS20(text, algo)\n","  else:\n","      ans = predictHybrid(text, algo)\n","  print(ans)\n","  answer=json.dumps(ans)\n","  res=make_response(answer)\n","  res.headers['Access-Control-Allow-Origin'] = '*'\n","  res.headers['Access-Control-Allow-Headers']= \"Origin, X-Requested-With, Content-Type, Accept\"\n","  return res\n","app.run()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting flask-ngrok\n","  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"],"name":"stdout"},{"output_type":"stream","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"],"name":"stderr"},{"output_type":"stream","text":[" * Running on http://bb1dc8950892.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n","Circulating on social networks a video that shows an excerpt from a Spanish television show, supposedly issued December 24, 2019, in which it appears a\n","woman (who claims to be psychic) to make \"predictions\". In this video, the woman describes a set of events that have been interpreted as a detailed forecast of\n","Covid-19 pandemic that has hit the world. It is, however, a fake video, at least as regards the date of issue. The video has been being disseminated on the\n","Internet with a date and not tam\n","pered with the real.\n","Regressor\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [13/May/2021 14:10:03] \"\u001b[37mGET /?algo=Regressor&granularity=Hybrid&text=Q2lyY3VsYXRpbmcgb24gc29jaWFsIG5ldHdvcmtzIGEgdmlkZW8gdGhhdCBzaG93cyBhbiBleGNlcnB0IGZyb20gYSBTcGFuaXNoIHRlbGV2aXNpb24gc2hvdywgc3VwcG9zZWRseSBpc3N1ZWQgRGVjZW1iZXIgMjQsIDIwMTksIGluIHdoaWNoIGl0IGFwcGVhcnMgYQp3b21hbiAod2hvIGNsYWltcyB0byBiZSBwc3ljaGljKSB0byBtYWtlICJwcmVkaWN0aW9ucyIuIEluIHRoaXMgdmlkZW8sIHRoZSB3b21hbiBkZXNjcmliZXMgYSBzZXQgb2YgZXZlbnRzIHRoYXQgaGF2ZSBiZWVuIGludGVycHJldGVkIGFzIGEgZGV0YWlsZWQgZm9yZWNhc3Qgb2YKQ292aWQtMTkgcGFuZGVtaWMgdGhhdCBoYXMgaGl0IHRoZSB3b3JsZC4gSXQgaXMsIGhvd2V2ZXIsIGEgZmFrZSB2aWRlbywgYXQgbGVhc3QgYXMgcmVnYXJkcyB0aGUgZGF0ZSBvZiBpc3N1ZS4gVGhlIHZpZGVvIGhhcyBiZWVuIGJlaW5nIGRpc3NlbWluYXRlZCBvbiB0aGUKSW50ZXJuZXQgd2l0aCBhIGRhdGUgYW5kIG5vdCB0YW0KcGVyZWQgd2l0aCB0aGUgcmVhbC4%3D HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["0\n","Fake\n"],"name":"stdout"},{"output_type":"stream","text":["127.0.0.1 - - [13/May/2021 14:10:09] \"\u001b[37mGET /?algo=Regressor&granularity=Fine&text=Q2lyY3VsYXRpbmcgb24gc29jaWFsIG5ldHdvcmtzIGEgdmlkZW8gdGhhdCBzaG93cyBhbiBleGNlcnB0IGZyb20gYSBTcGFuaXNoIHRlbGV2aXNpb24gc2hvdywgc3VwcG9zZWRseSBpc3N1ZWQgRGVjZW1iZXIgMjQsIDIwMTksIGluIHdoaWNoIGl0IGFwcGVhcnMgYQp3b21hbiAod2hvIGNsYWltcyB0byBiZSBwc3ljaGljKSB0byBtYWtlICJwcmVkaWN0aW9ucyIuIEluIHRoaXMgdmlkZW8sIHRoZSB3b21hbiBkZXNjcmliZXMgYSBzZXQgb2YgZXZlbnRzIHRoYXQgaGF2ZSBiZWVuIGludGVycHJldGVkIGFzIGEgZGV0YWlsZWQgZm9yZWNhc3Qgb2YKQ292aWQtMTkgcGFuZGVtaWMgdGhhdCBoYXMgaGl0IHRoZSB3b3JsZC4gSXQgaXMsIGhvd2V2ZXIsIGEgZmFrZSB2aWRlbywgYXQgbGVhc3QgYXMgcmVnYXJkcyB0aGUgZGF0ZSBvZiBpc3N1ZS4gVGhlIHZpZGVvIGhhcyBiZWVuIGJlaW5nIGRpc3NlbWluYXRlZCBvbiB0aGUKSW50ZXJuZXQgd2l0aCBhIGRhdGUgYW5kIG5vdCB0YW0KcGVyZWQgd2l0aCB0aGUgcmVhbC4%3D HTTP/1.1\u001b[0m\" 200 -\n"],"name":"stderr"},{"output_type":"stream","text":["Circulating on social networks a video that shows an excerpt from a Spanish television show, supposedly issued December 24, 2019, in which it appears a\n","woman (who claims to be psychic) to make \"predictions\". In this video, the woman describes a set of events that have been interpreted as a detailed forecast of\n","Covid-19 pandemic that has hit the world. It is, however, a fake video, at least as regards the date of issue. The video has been being disseminated on the\n","Internet with a date and not tam\n","pered with the real.\n","Regressor\n","0 help office dance money  ...    writing rural positive_emotion    musical\n","0    0      0     0     0  ...  0.0434783     0                0  0.0869565\n","\n","[1 rows x 194 columns]\n","0\n","Fake\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CsQomsOmQ-Fn"},"source":["predictTS20('philippin presid rodrigo dutert warn peopl flout countri lockdown slow spread coronaviru could shoot april al jazeera report polic kill man refus follow restrict queen elizabeth offer word support presid follow day facebook post claim messag hope amid coronaviru pandem queen call dutert kind leader know way filipino fortun quot continu serious look overwork may perfect truli love countri philippin worst govern actual worst citizen post flag part facebook effort combat fals news misinform news fee read partnership facebook featur fact check blogger state april blog post joe exot test posit coronaviru prison confirm ciara rourk april queen elizabeth deliv rare speech april april like facebook post say thank unit kingdom resid stay insid vow would succeed fight coronaviru pandem mention dutert philippin search quot onlin find credibl sourc show say elsewher either rate facebook post pant fire', 'Regressor')"],"execution_count":null,"outputs":[]}]}